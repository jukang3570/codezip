{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from check_grad import check_grad\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(weights, data):\n",
    "    data_with_bias = np.hstack((data, np.ones((data.shape[0], 1))))\n",
    "\n",
    "    logit = np.dot(data_with_bias, weights)\n",
    "\n",
    "    y = sigmoid(logit)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(targets, y):\n",
    "    ce = -np.mean(targets * np.log(y) + (1 - targets) * np.log(1 - y))\n",
    "\n",
    "    predictions = (y >= 0.5).astype(int)\n",
    "\n",
    "    frac_correct = np.mean(predictions == targets)\n",
    "\n",
    "    return ce, frac_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(weights, data, targets, hyperparameters):\n",
    "\n",
    "    y = logistic_predict(weights, data)\n",
    "\n",
    "    if hyperparameters['weight_regularization'] is True:\n",
    "        f, df = logistic_pen(weights, data, targets, hyperparameters)\n",
    "    else:\n",
    "        f = -np.mean(targets * np.log(y) + (1 - targets) * np.log(1 - y))\n",
    "\n",
    "        data_with_bias = np.hstack((data, np.ones((data.shape[0], 1))))\n",
    "        df = np.dot(data_with_bias.T, (y - targets))\n",
    "\n",
    "    return f, df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_pen(weights, data, targets, hyperparameters):\n",
    "    N = len(targets)\n",
    "    M = len(weights) - 1  \n",
    "\n",
    "    data_with_bias = np.hstack((data, np.ones((N, 1))))\n",
    "\n",
    "    logit = np.dot(data_with_bias, weights)\n",
    "\n",
    "    y = sigmoid(logit)\n",
    "\n",
    "    f = -np.mean(targets * np.log(y) + (1 - targets) * np.log(1 - y))\n",
    "\n",
    "    df = np.dot(data_with_bias.T, (y - targets))\n",
    "\n",
    "    reg_term = (hyperparameters['weight_decay'] / 2) * np.sum(weights[:-1] ** 2)\n",
    "\n",
    "    f += reg_term\n",
    "    df[:-1] += hyperparameters['weight_decay'] * weights[:-1]\n",
    "\n",
    "    return f, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparameters):\n",
    "    print_interval = hyperparameters.setdefault('print_interval', 1)\n",
    "    if hyperparameters['mnist_train_small']:\n",
    "        train_inputs, train_targets = load_train_small()\n",
    "    else:\n",
    "        train_inputs, train_targets = load_train()\n",
    "\n",
    "    valid_inputs, valid_targets = load_valid()\n",
    "\n",
    "    N, M = train_inputs.shape\n",
    "\n",
    "    weights = np.random.randn(M + 1, 1)\n",
    "\n",
    "    run_check_grad(hyperparameters)\n",
    "\n",
    "    logging = np.zeros((hyperparameters['num_iterations'], 5))\n",
    "    for t in range(hyperparameters['num_iterations']):\n",
    "\n",
    "        f, df, predictions = logistic(weights, train_inputs, train_targets, hyperparameters)\n",
    "        \n",
    "        cross_entropy_train, frac_correct_train = evaluate(train_targets, predictions)\n",
    "\n",
    "        if np.isnan(f) or np.isinf(f):\n",
    "            raise ValueError(\"nan/inf error\")\n",
    "\n",
    "        weights = weights - hyperparameters['learning_rate'] * df / N\n",
    "\n",
    "        predictions_valid = logistic_predict(weights, valid_inputs)\n",
    "\n",
    "        cross_entropy_valid, frac_correct_valid = evaluate(valid_targets, predictions_valid)\n",
    "        \n",
    "        logging[t] = [f / N, cross_entropy_train, frac_correct_train*100, cross_entropy_valid, frac_correct_valid*100]\n",
    "        \n",
    "        if t % print_interval != 0:\n",
    "            continue\n",
    "        print(f\"ITERATION:{t+1:4d}  \"\n",
    "              f\"TRAIN NLOGL:{f / N:4.2f}  \"\n",
    "              f\"TRAIN CE:{cross_entropy_train:.6f}  \"\n",
    "              f\"TRAIN FRAC:{frac_correct_train*100:5.1f}  \"\n",
    "              f\"VALID CE:{cross_entropy_valid:.6f}   \"\n",
    "              f\"VALID FRAC:{frac_correct_valid*100:5.1f}\")\n",
    "        \n",
    "    return logging, weights\n",
    "\n",
    "def run_check_grad(hyperparameters):\n",
    "    \"\"\"Performs gradient check on logistic function.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    num_examples = 7\n",
    "    num_dimensions = 9\n",
    "\n",
    "    weights = np.random.randn(num_dimensions+1, 1)\n",
    "    data    = np.random.randn(num_examples, num_dimensions)\n",
    "    targets = (np.random.rand(num_examples, 1) > 0.5).astype(int)\n",
    "\n",
    "    diff = check_grad(logistic,      \n",
    "                      weights,\n",
    "                      0.001,         \n",
    "                      data,\n",
    "                      targets,\n",
    "                      hyperparameters)\n",
    "\n",
    "    print(\"diff =\", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(hyperparameters):\n",
    "    num_runs = 1\n",
    "    weights = []\n",
    "    logging = np.zeros((hyperparameters['num_iterations'], 5))\n",
    "    for i in range(num_runs):\n",
    "        _logging, _weights = train(hyperparameters)\n",
    "        logging += _logging\n",
    "        weights.append(_weights)\n",
    "    logging /= num_runs\n",
    "\n",
    "    plot_loss_curve(logging)\n",
    "    plot_accuracy_curve(logging)\n",
    "    \n",
    "    return logging, weights\n",
    "\n",
    "def plot_loss_curve(logging):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(1, logging.shape[0] + 1), logging[:, 1], label='Training Loss')\n",
    "    plt.plot(range(1, logging.shape[0] + 1), logging[:, 3], label='Validation Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curve(logging):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(1, logging.shape[0] + 1), logging[:, 2], label='Training Accuracy')\n",
    "    plt.plot(range(1, logging.shape[0] + 1), logging[:, 4], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_test(weights: list):\n",
    "    inputs, targets = load_test()\n",
    "    \n",
    "    for i, weight in enumerate(weights, 1):\n",
    "        predictions = logistic_predict(weight, inputs)\n",
    "        cross_entropy, frac_correct = evaluate(targets, predictions)\n",
    "        print(f'WEIGHTS #{i}   TEST CE: {cross_entropy:.6f}   TEST FRAC: {frac_correct*100:5.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'mnist_train_small': False,      # boolean, True for using small dataset\n",
    "    'learning_rate': 0.01,          # learning rate for gradient descent\n",
    "    'weight_regularization': True,   # boolean, True for using regularization\n",
    "    'num_iterations': 1000,           # number of iterations for training\n",
    "    'weight_decay': 0.1,           # regularization strength lambda\n",
    "    'print_interval': 1              # adjust print interval higher if the number of iterations is large. not affects logging\n",
    "}\n",
    "\n",
    "logging, trained_weights = run_train(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic_regression'></a>\n",
    "#### Logistic Regression\n",
    "#### mnist_train과 mnist_train_small의 두종류 데이터 집합 분석 및 평가\n",
    "\n",
    "hyperparameters = {</br>\n",
    "    </br>'mnist_train_small': False & True      \n",
    "    </br>'learning_rate': 0.01,       #최적의 파라미터\n",
    "    </br>'weight_regularization': False,  \n",
    "    </br>'num_iterations': 1000,          #최적의 파라미터\n",
    "    </br>'weight_decay': 0.1,         \n",
    "    </br>'print_interval': 1          \n",
    "}\n",
    "</br>\n",
    "\n",
    "</br>\n",
    "ITERATION:1000  TRAIN NLOGL:0.00  TRAIN CE:0.459104  TRAIN FRAC: 86.5  VALID CE:0.820872   VALID FRAC: 80.0\n",
    "\n",
    "#### mnist_train data\n",
    "![plot sample](./plot_sample01_png.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mnist_train_small\n",
    "\n",
    "ITERATION:1000  TRAIN NLOGL:0.00  TRAIN CE:0.011276  TRAIN FRAC:100.0  VALID CE:2.309556   VALID FRAC: 64.0\n",
    "![plot sample](./image02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mnist_train_small 같은 경우 데이터 셋이 상대적으로 mani_train 보다 적기 때문에 크로스 엔트로피 손실이 낮다. \n",
    "\n",
    "- 데이터 셋이 적기 때문에 mnist_train_small 데이터 셋은 훈련정확도가 높지만 학습 데이터에 의존성이 커 Overfitting의 성향을 나타내고 있다. 이에 따라 validation accurcy에서 mnist_train_small 은 약한 모습을 보이고 있다.\n",
    "\n",
    "- mnist_train 이 mnist_train_small 보다 일반화 능력이 높으며 안정적인 성능을 보이고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 반복 평가\n",
    "\n",
    "ITERATION:1000  TRAIN NLOGL:0.00  TRAIN CE:0.459648  TRAIN FRAC: 88.0  VALID CE:0.920228   VALID FRAC: 78.0\n",
    "![plot sample](./image03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5회 이하 정도의 훈련 반복시 일시적으로 정확도의 값이 올라간다. 하지만 훈련이 계속 반복되는 경우 위의그래프처럼 성능이 오히려 떨어지는 경향을 보이고 있다.\n",
    "\n",
    "- 정규화를 적용하지 않았기 때문에 훈련을 진행할수록 모델이 훈련 데이터에 맞춰줘 train data에만 특화가 되는 Overfitting 현상이 일어난다. 이로 인해 새로운 데이터에 대한 일반화 능력이 떨어지게 된다.\n",
    "\n",
    "- Gradient Descent가 수렴하지 못하고 발산하기 때문에 loss 함수의 최소값을 찾지 못하고 무한히 큰 loss 함수를 가지게 될 수 있어 모델의 성능이 저하된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularized_logistic_regression'></a>\n",
    "#### Regularized Logistic Regression\n",
    "\n",
    "hyperparameters = {</br>\n",
    "    </br>'mnist_train_small': False # 최적의 파라민터     \n",
    "    </br>'learning_rate': 0.01,       #최적의 파라미터\n",
    "    </br>'weight_regularization': True,  #regularized_logistic 적용\n",
    "    </br>'num_iterations': 1000,          #최적의 파라미터\n",
    "    </br>'weight_decay': 1 & 0.1 & 0.01 & 0.001         \n",
    "    </br>'print_interval': 1          \n",
    "}\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\lambda$ = 1\n",
    "ITERATION:1000  TRAIN NLOGL:0.19  TRAIN CE:0.402023  TRAIN FRAC: 84.5  VALID CE:1.279483   VALID FRAC: 72.0\n",
    "![plot sample](./image04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\lambda$ = 0.1\n",
    "ITERATION:1000  TRAIN NLOGL:0.20  TRAIN CE:0.300512  TRAIN FRAC: 90.5  VALID CE:0.842592   VALID FRAC: 74.0\n",
    "![plot sample](./image07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\lambda$ = 0.01\n",
    "ITERATION:1000  TRAIN NLOGL:0.02  TRAIN CE:0.354615  TRAIN FRAC: 88.0  VALID CE:0.935801   VALID FRAC: 78.0\n",
    "![plot sample](./image05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\lambda$ = 0.001\n",
    "ITERATION:1000  TRAIN NLOGL:0.00  TRAIN CE:0.479517  TRAIN FRAC: 86.5  VALID CE:0.559413   VALID FRAC: 84.0\n",
    "![plot sample](./image06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$ 값이 낮아질수록 엔트로피 손실 값은 낮아지며 훈련 데이터와 검증 데이터의 정확도가 증가하고 있다.\n",
    "\n",
    "- $\\lambda$ 낮아질수록 regularization의 강도를 약화시켜 복잡한 패턴으로 훈련을 할 수 있게 된다. 즉 모델의 복잡성이 증가하게 되어 validation accuracy가 증가하는 모습을 볼 수 있다.\n",
    "\n",
    "- 위를 통하여 현재 모델의 최적의 $\\lambda$ = 0.001 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic_regression 과 regularized_logistic_regression 성능비교\n",
    "\n",
    "-  regularized_logistic_regression\n",
    "</br>\n",
    "\n",
    "</br> WEIGHTS #1   TEST CE: 0.655075   TEST FRAC:  82.0 </br>\n",
    "\n",
    "-  logistic_regression\n",
    "</br> \n",
    "\n",
    "</br> WEIGHTS #1   TEST CE: 0.741427   TEST FRAC:  74.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두개의 성능 평가를 보게 되면 새로운 test data set에 대해 regularized 된 logistic regression 이 더 정확도와 cross entropy 가 더 우수하게 나왔다.\n",
    "\n",
    "- logistic regression 의 loss 함수는 regularzation이 없이 학습을 진행하였기 때문에 모델이 훈련 데이터에 overfitting 되기가 쉽다는 단점이 있다. \n",
    "\n",
    "- regularized-logistic 은가중치의 페널티를 부여하여 모델이 일반화하는데 도움을 준다.\n",
    "그렇기 때문에 지금까지 보지 못한 test-set에 데이터는 regularized-logistic-regression이 우수한 성능을 보일 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
